{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c74055-69f4-439a-87af-119d062a707a",
   "metadata": {},
   "source": [
    "# Detection of faces and cropping using predefined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce61d8f-5ec3-4bec-a195-12e573fa9713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import dlib\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a576d067-b037-4a29-bb72-1bd56d07af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"G:\\my-proj\\colour-analysis-profession\\mixed_photos\\Light\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4451229e-6533-42e7-85d5-9b5935b80e12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif'))]\n",
    "len(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47600322-e79b-447b-9945-e3190f93f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dlib's CNN-based face detector\n",
    "cnn_detector = dlib.cnn_face_detection_model_v1(\"mmod_human_face_detector.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4168a6c0-551e-4e5f-a6f9-33986d187a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_regions(image_path):\n",
    "    \"\"\"\n",
    "    Extracts the face regions from the detected faces.\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "\n",
    "    # Read the image\n",
    "    image = cv.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Failed to load the image. Check the file format or path: {image_path}\")\n",
    "    \n",
    "    # Convert to RGB\n",
    "    rgb_image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Detect faces using the CNN detector (assuming cnn_detector is defined)\n",
    "    faces_cnn = cnn_detector(rgb_image)\n",
    "    \n",
    "    # Extract face regions\n",
    "    face_regions = []\n",
    "    for face in faces_cnn:\n",
    "        x, y, w, h = (face.rect.left(), face.rect.top(), face.rect.width(), face.rect.height())\n",
    "        face_crop = image[y:y+h, x:x+w]  # Crop the face region\n",
    "        face_regions.append(face_crop)\n",
    "    \n",
    "    return face_regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ee9a87e-0f59-40d3-bee1-258c3c2290fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 31s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cropped_faces=[]\n",
    "for i in image_files:\n",
    "    cropped_faces.extend(extract_face_regions(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99192ce2-7d80-414c-9959-826e6d2c1e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping empty or invalid cropped face at index 15\n",
      "Saved 34 valid cropped faces to the folder: l1\n"
     ]
    }
   ],
   "source": [
    "# Directory to save cropped faces\n",
    "output_dir = \"l1\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Save each cropped face as an image\n",
    "saved_count = 0\n",
    "for idx, face in enumerate(cropped_faces):\n",
    "    if face is None or face.size == 0:  # Check if the face is empty\n",
    "        print(f\"Skipping empty or invalid cropped face at index {idx}\")\n",
    "        continue\n",
    "\n",
    "    # Construct the output file path\n",
    "    output_path = os.path.join(output_dir, f\"face_{idx + 1}.jpg\")\n",
    "    \n",
    "    # Save the image using OpenCV\n",
    "    cv.imwrite(output_path, face)\n",
    "    saved_count += 1\n",
    "\n",
    "print(f\"Saved {saved_count} valid cropped faces to the folder: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "652f2dde-1a25-4313-bdc0-5c3681d9ac94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_23244\\2505560897.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"resnet_model.pth\"))\n",
      "2024-12-08 21:59:29.162 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "import dlib\n",
    "import os\n",
    "\n",
    "# Load pre-trained ResNet model for skin tone prediction\n",
    "model = resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 3)  # 3 classes: Dark, Light, Medium\n",
    "model.load_state_dict(torch.load(\"resnet_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Define the same transformations used during training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the dlib CNN face detector\n",
    "cnn_detector = dlib.cnn_face_detection_model_v1(\"mmod_human_face_detector.dat\")\n",
    "\n",
    "# Extended dataset with color names and hex codes\n",
    "data = {\n",
    "    \"Skin Color Type\": [\"Light\", \"Medium\", \"Dark\"],\n",
    "    \"Colour Pattern\": [\n",
    "        \"Bright and pastel shades\",\n",
    "        \"Warm and earthy tones\",\n",
    "        \"Rich and bold colors\"\n",
    "    ],\n",
    "    \"Single Colour\": [\n",
    "        [(\"Lavender\", \"#E6E6FA\"), (\"Mint\", \"#98FF98\"), (\"Sky Blue\", \"#87CEEB\"), (\"Peach\", \"#FFDAB9\"), (\"Powder Pink\", \"#FFC0CB\"), (\"Soft Yellow\", \"#FFFACD\"), (\"Ivory\", \"#FFFFF0\")],\n",
    "        [(\"Olive\", \"#808000\"), (\"Coral\", \"#FF7F50\"), (\"Beige\", \"#F5F5DC\"), (\"Burnt Orange\", \"#CC5500\"), (\"Mustard Yellow\", \"#FFDB58\"), (\"Turquoise\", \"#40E0D0\"), (\"Cinnamon Brown\", \"#7B3F00\")],\n",
    "        [(\"Maroon\", \"#800000\"), (\"Navy Blue\", \"#000080\"), (\"Emerald Green\", \"#50C878\"), (\"Gold\", \"#FFD700\"), (\"Deep Purple\", \"#673AB7\"), (\"Ruby Red\", \"#9B111E\"), (\"Charcoal Gray\", \"#36454F\")]\n",
    "    ],\n",
    "    \"Dual Colour\": [\n",
    "        [(\"Lavender + Mint\", [\"#E6E6FA\", \"#98FF98\"]), (\"Sky Blue + Peach\", [\"#87CEEB\", \"#FFDAB9\"]), (\"Soft Yellow + Powder Pink\", [\"#FFFACD\", \"#FFC0CB\"])],\n",
    "        [(\"Olive + Coral\", [\"#808000\", \"#FF7F50\"]), (\"Beige + Burnt Orange\", [\"#F5F5DC\", \"#CC5500\"]), (\"Turquoise + Mustard Yellow\", [\"#40E0D0\", \"#FFDB58\"])],\n",
    "        [(\"Maroon + Navy Blue\", [\"#800000\", \"#000080\"]), (\"Emerald Green + Gold\", [\"#50C878\", \"#FFD700\"]), (\"Deep Purple + Ruby Red\", [\"#673AB7\", \"#9B111E\"])]\n",
    "    ],\n",
    "    \"Multicolour\": [\n",
    "        [(\"Lavender + Mint + Sky Blue\", [\"#E6E6FA\", \"#98FF98\", \"#87CEEB\"])],\n",
    "        [(\"Olive + Coral + Beige\", [\"#808000\", \"#FF7F50\", \"#F5F5DC\"])],\n",
    "        [(\"Maroon + Navy Blue + Emerald Green\", [\"#800000\", \"#000080\", \"#50C878\"])]\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Function to detect faces and extract face regions\n",
    "def extract_face_regions(image_path):\n",
    "    # Read the image\n",
    "    image = cv.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Failed to load the image. Check the file format or path: {image_path}\")\n",
    "    \n",
    "    # Convert to RGB\n",
    "    rgb_image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Detect faces\n",
    "    faces_cnn = cnn_detector(rgb_image)\n",
    "    \n",
    "    # Extract face regions\n",
    "    face_regions = []\n",
    "    for face in faces_cnn:\n",
    "        x, y, w, h = (face.rect.left(), face.rect.top(), face.rect.width(), face.rect.height())\n",
    "        face_crop = image[y:y+h, x:x+w]\n",
    "        face_regions.append(face_crop)\n",
    "    \n",
    "    return face_regions\n",
    "\n",
    "# Function to predict skin tone for a given face region\n",
    "def predict_skin_tone(face_region):\n",
    "    # Convert the face region to PIL format\n",
    "    face_image = Image.fromarray(cv.cvtColor(face_region, cv.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Preprocess the image\n",
    "    image_tensor = transform(face_image).unsqueeze(0)\n",
    "    \n",
    "    # Predict skin tone\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    class_names = ['Dark', 'Light', 'Medium']  # Match folder names\n",
    "    return class_names[predicted.item()]\n",
    "\n",
    "# Function to render colors as blocks\n",
    "def render_colors(color_data):\n",
    "    for name, colors in color_data:\n",
    "        if isinstance(colors, list):  # For dual or multicolor palettes\n",
    "            st.write(f\"**{name}:**\")\n",
    "            cols = st.columns(len(colors))\n",
    "            for idx, color in enumerate(colors):\n",
    "                cols[idx].markdown(f'<div style=\"width: 50px; height: 25px; background-color: {color};\"></div>', unsafe_allow_html=True)\n",
    "        else:  # For single colors\n",
    "            st.write(f\"**{name}:**\")\n",
    "            st.markdown(f'<div style=\"width: 50px; height: 25px; background-color: {colors};\"></div>', unsafe_allow_html=True)\n",
    "\n",
    "# Streamlit application\n",
    "st.title(\"Skin Tone Color Analysis\")\n",
    "\n",
    "# Upload an image\n",
    "uploaded_image = st.file_uploader(\"Upload an image to analyze skin tone:\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "\n",
    "if uploaded_image:\n",
    "    # Save uploaded image\n",
    "    st.image(uploaded_image, caption=\"Uploaded Image\", use_column_width=True)\n",
    "    with open(\"uploaded_image.jpg\", \"wb\") as f:\n",
    "        f.write(uploaded_image.getbuffer())\n",
    "    \n",
    "    # Detect face regions\n",
    "    face_regions = extract_face_regions(\"uploaded_image.jpg\")\n",
    "    if not face_regions:\n",
    "        st.error(\"No faces detected in the uploaded image.\")\n",
    "    else:\n",
    "        # Predict skin tone for the first detected face\n",
    "        skin_tone = predict_skin_tone(face_regions[0])\n",
    "        st.write(f\"**Predicted Skin Tone:** {skin_tone}\")\n",
    "        \n",
    "        # Display best colors for the predicted skin tone\n",
    "        index = data[\"Skin Color Type\"].index(skin_tone)\n",
    "        \n",
    "        st.write(f\"### Best Colors for {skin_tone} Skin Tone\")\n",
    "        st.write(f\"**Colour Pattern:** {data['Colour Pattern'][index]}\")\n",
    "        \n",
    "        st.write(\"### Single Colours\")\n",
    "        render_colors(data[\"Single Colour\"][index])\n",
    "        \n",
    "        st.write(\"### Dual Colours\")\n",
    "        render_colors(data[\"Dual Colour\"][index])\n",
    "        \n",
    "        st.write(\"### Multicolours\")\n",
    "        render_colors(data[\"Multicolour\"][index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0fd02f8-8f1e-4b05-bb82-ad1b412a4ba2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predicted\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predicted' is not defined"
     ]
    }
   ],
   "source": [
    "predicted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
